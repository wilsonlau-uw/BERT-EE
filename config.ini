
[general]
seed=23
# ner or re  or  ner,re
tasks=  ner,re

#CRITICAL, ERROR, WARNING,  INFO, DEBUG
log_level=INFO
multi_gpu = True
# train or predict
mode = train
results_path = ./results
# use-specified folder name.  If not provided, default name is <task>_timestamp
results_folder =
use_fine_tuned = False
# path of fine-tune model, applicable only if use_fine_tuned = True
fine_tuned_path=
# True if input text needs to be segmented into sentences.  False if input text has been segmented into sentences.
use_sent_segmentation=True
# True if using spacy tokenizer. False if tokenizing by space
use_spacy_tokenizer=False
spacy_model=en_core_web_lg

# this section is for named entity recognition (NER) task configuration
[ner]
# comma delimited list of ner labels included in training. If not provided, all annotated ner labels would be included.
labels =
scheme=IOB2

# folder path of annotated training data in BRAT .ann format
training_folder =
# folder path of annotated validation data in BRAT .ann format
validation_folder =
# folder path of text (.txt) files for prediction. Applicable if model=predict
prediction_folder =

dropout=0.1
eval_mode=None

# this section is for relation extraction (RE) task configuration
[re]
# comma delimited list of re labels included in training. If not provided, all annotated re labels would be included.
labels =
# folder path of annotated training data in BRAT .ann format
training_folder =
# folder path of annotated validation data in BRAT .ann format
validation_folder =
# folder path of BRAT .ann data which should already contain NER labels.
# if none is provided, the predicted NER results would be used for RE prediction. Applicable if model=predict
prediction_folder =
dropout=0.1

# number of characters between two entities in a relation.
# If not set, the default is the median distance of all relations in labelled dataset
distance=
# limiting the number of no_relation as a percentage of total number of events
no_relation_ratio=
num_workers=8
# number of sentences in which the relations would be considered.
max_sent_windows=1

# this section is for model configuration
[model]
max_seq_len=128
pretrained_model_name_or_path = bert-base-uncased
tokenizer_lower_case = True
batch=64
epochs=100
patience=10
grad_clipping=1.0
learning_rate=2e-5
weight_decay=0.0
adam_epsilon=1e-6
warmup_proportion =0.1
warmup_step=
#linear or ms or rop or exp or none
scheduler_type=
learning_rate_decay_factor = 0.5
